{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of sentiment_final.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"7d991--97s6M","colab_type":"code","colab":{}},"source":["# 1.) function for reading data and formatting\n","\n","def read_file(file_name): \n","    data_list  = []\n","    with open(file_name, 'r') as f: \n","        for line in f: \n","            line = line.strip() \n","            label = ' '.join(line[1:line.find(\"]\")].strip().split())\n","            text = line[line.find(\"]\")+1:].strip()\n","            data_list.append([label, text])\n","    return data_list "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KX5nWNFt8Tv8","colab_type":"code","colab":{}},"source":["# file path\n","\n","file_name = \"../content/psychExp.txt\"\n","psychExp_txt = read_file(file_name)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hrq5jLiB9b1x","colab_type":"code","outputId":"6dbaef40-c284-48ce-e39e-7067a7ba2a14","executionInfo":{"status":"ok","timestamp":1558935773707,"user_tz":-330,"elapsed":2813,"user":{"displayName":"GAGAN SINGH SAINI","photoUrl":"","userId":"08621075158641464912"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print('Total no. of instances: {}'.format(len(psychExp_txt)))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Total no. of instances: 7480\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mbzMka0m9iMK","colab_type":"code","colab":{}},"source":["# method for converting labels from one hot to classes\n","\n","def convert_label(item, name): \n","    items = list(map(float, item.split()))\n","    label = \"\"\n","    for idx in range(len(items)): \n","        if items[idx] == 1: \n","            label += name[idx] + \" \"\n","    \n","    return label.strip()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IqWMWTPg9yUZ","colab_type":"code","colab":{}},"source":["# classes\n","\n","emotions = [\"joy\", 'fear', \"anger\", \"sadness\", \"disgust\", \"shame\", \"guilt\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EGNK_nnJ92kL","colab_type":"code","colab":{}},"source":["X_all = []\n","y_all = []\n","for label, text in psychExp_txt:\n","    X_all.append(text.lower())\n","    y_all.append(convert_label(label, emotions))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pRcy55bR-HEj","colab_type":"code","outputId":"78c06385-2f81-4124-9760-fe88b23f1578","executionInfo":{"status":"ok","timestamp":1558935773716,"user_tz":-330,"elapsed":2779,"user":{"displayName":"GAGAN SINGH SAINI","photoUrl":"","userId":"08621075158641464912"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Data processing\n","\n","from string import punctuation\n","print('punctuations to be removed: {}'.format(punctuation))\n","\n","X_all_pro = []\n","for text in X_all:\n","    all_text = ''.join([c for c in text if c not in punctuation])\n","    X_all_pro.append(all_text)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["punctuations to be removed: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tKpeKerW-IIS","colab_type":"code","colab":{}},"source":["# Tokenize — Create Vocab to Int mapping dictionary\n","\n","from collections import Counter\n","all_text2 = ' '.join(X_all_pro)\n","# create a list of words\n","words = all_text2.split()\n","# Count all the words using Counter Method\n","count_words = Counter(words)\n","\n","total_words = len(words)\n","sorted_words = count_words.most_common(total_words)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zUTahaws-QxQ","colab_type":"code","colab":{}},"source":["# vocab to integer mapping dictionary with starting index 1\n","\n","vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iVnXDeP7-Uni","colab_type":"code","outputId":"c351d6b3-3fbd-4c77-9183-ef957bb6fd9c","executionInfo":{"status":"ok","timestamp":1558935773720,"user_tz":-330,"elapsed":2747,"user":{"displayName":"GAGAN SINGH SAINI","photoUrl":"","userId":"08621075158641464912"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# encoading the words\n","\n","X_all_int = []\n","for text in X_all_pro:\n","    r = [vocab_to_int[w] for w in text.split()]\n","    X_all_int.append(r)\n","    \n","print (X_all_int[0:3])"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[[112, 2, 572, 9, 952, 10, 161, 256, 35, 13, 27, 169, 6, 540, 8, 27, 11, 15, 169, 14, 3, 150, 35], [8, 1, 7, 414, 10, 3, 772, 192], [8, 1, 7, 326, 50, 33, 189, 177, 9, 347, 114, 63, 7, 3, 4683, 1363, 9, 12, 41, 7, 326, 18, 1462, 4684, 6, 283, 550, 43, 606, 4685, 4, 314, 12, 2282]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gPw-Hh7F-fqA","colab_type":"code","colab":{}},"source":["# for convering labels from classes to class index\n","def label_to_int(label_text):\n","    emotions_dict = {\"joy\":0, \"fear\":1, \"anger\":2, \"sadness\":3, \"disgust\":4, \"shame\":5, \"guilt\":6}\n","    label_int = []\n","    for label in label_text:\n","        label_int.append(emotions_dict[label])\n","    return label_int"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAyyWsfd_UVI","colab_type":"code","colab":{}},"source":["labels = label_to_int(y_all)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ASYEsJaV-vVo","colab_type":"code","outputId":"3c6785e6-cab3-493d-d87e-4b01a305f0e0","executionInfo":{"status":"ok","timestamp":1558935773724,"user_tz":-330,"elapsed":2718,"user":{"displayName":"GAGAN SINGH SAINI","photoUrl":"","userId":"08621075158641464912"}},"colab":{"base_uri":"https://localhost:8080/","height":420}},"source":["# analysing the text data lenghth\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","text_len = [len(x) for x in X_all_int]\n","pd.Series(text_len).hist()\n","plt.show()\n","pd.Series(text_len).describe()"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["count    7480.000000\n","mean       22.160160\n","std        14.654072\n","min         1.000000\n","25%        12.000000\n","50%        19.000000\n","75%        30.000000\n","max       178.000000\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"huGUlqzg-xTg","colab_type":"code","colab":{}},"source":["# padding the data for having same dimention\n","import numpy as np\n","\n","def pad_features(text_int, seq_length):\n","    ''' Return features of text_ints, where each text is padded with 0's or truncated to the input seq_length.\n","    '''\n","    features = np.zeros((len(text_int), seq_length), dtype = int)\n","    \n","    for i, text in enumerate(text_int):\n","        text_len = len(text)\n","        \n","        if text_len <= seq_length:\n","            zeroes = list(np.zeros(seq_length-text_len))\n","            new = zeroes+text\n","        elif text_len > seq_length:\n","            new = text[0:seq_length]\n","        \n","        features[i,:] = np.array(new)\n","    \n","    return features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RONdLDac_eSi","colab_type":"code","colab":{}},"source":["seq_length = 50 # length after padding \n","X_all_pad = pad_features(X_all_int, seq_length)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4jjYkm0A-2IQ","colab_type":"code","outputId":"39ffff86-3b83-425f-9742-c40828497bed","executionInfo":{"status":"ok","timestamp":1558935773732,"user_tz":-330,"elapsed":2699,"user":{"displayName":"GAGAN SINGH SAINI","photoUrl":"","userId":"08621075158641464912"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["print('length of X_all_pad: {}'.format(len(X_all_pad)))\n","print('length of labels: {}'.format(len(labels)))\n","\n","# converting labels from list to numpy.nparray\n","\n","print('data type of labels before convesion: {}'.format(type(labels)))\n","labels = np.array(labels)\n","print('data type of labels after convesion: {}'.format(type(labels)))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["length of X_all_pad: 7480\n","length of labels: 7480\n","data type of labels before convesion: <class 'list'>\n","data type of labels after convesion: <class 'numpy.ndarray'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bGXPZVRg_q4v","colab_type":"code","outputId":"6be58c81-5dd5-4adb-91e0-58b5f489baf3","executionInfo":{"status":"ok","timestamp":1558935773733,"user_tz":-330,"elapsed":2688,"user":{"displayName":"GAGAN SINGH SAINI","photoUrl":"","userId":"08621075158641464912"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["# train (80%), validation (10%) ,test data (10%) split\n","\n","len_X_all = len(X_all_pad)\n","\n","\n","split_frac = 0.8\n","\n","train_x = X_all_pad[0:int(split_frac*len_X_all)+16]\n","train_y = labels[0:int(split_frac*len_X_all)+16]\n","\n","remaining_x = X_all_pad[int(split_frac*len_X_all)-4:]\n","remaining_y = labels[int(split_frac*len_X_all)-4:]\n","\n","test_x = remaining_x[0:int(len(remaining_x)*0.5)]\n","test_y = remaining_y[0:int(len(remaining_y)*0.5)]\n","\n","valid_x = remaining_x[int(len(remaining_x)*0.5):]\n","valid_y = remaining_y[int(len(remaining_y)*0.5):]\n","\n","print(type(train_x),type(np.array(train_y)))\n","\n","print(\"train data size: \",train_x.shape)\n","print(\"remaining data size: \",remaining_x.shape)\n","print(\"valid data size: \",valid_x.shape)\n","print(\"test data size: \",test_x.shape)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n","train data size:  (6000, 50)\n","remaining data size:  (1500, 50)\n","valid data size:  (750, 50)\n","test data size:  (750, 50)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZM2nCxYp_yMQ","colab_type":"code","colab":{}},"source":["# Data loading and batch formation\n","\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# create Tensor datasets\n","train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n","valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n","test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n","\n","# dataloaders\n","batch_size = 50\n","\n","# Shuffeling the data\n","train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n","test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7qm2HZ9_4eq","colab_type":"code","outputId":"855c07df-c29a-4f81-f3ba-ed773fc2760f","executionInfo":{"status":"ok","timestamp":1558935774222,"user_tz":-330,"elapsed":3157,"user":{"displayName":"GAGAN SINGH SAINI","photoUrl":"","userId":"08621075158641464912"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["# obtain one batch of training data\n","\n","dataiter = iter(train_loader)\n","sample_x, sample_y = dataiter.next()\n","print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n","print('Sample input: \\n', sample_x)\n","print()\n","print('Sample label size: ', sample_y.size()) # batch_size\n","print('Sample label: \\n', sample_y)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Sample input size:  torch.Size([50, 50])\n","Sample input: \n"," tensor([[   0,    0,    0,  ...,   14,   13,   65],\n","        [   0,    0,    0,  ...,    5, 7871,  102],\n","        [   0,    0,    0,  ...,    1,   64,   26],\n","        ...,\n","        [   0,    0,    0,  ...,  149,   10,  303],\n","        [   0,    0,    0,  ..., 1926,   19, 6938],\n","        [   0,    0,    0,  ...,   10,    3,  244]])\n","\n","Sample label size:  torch.Size([50])\n","Sample label: \n"," tensor([0, 2, 0, 3, 5, 3, 0, 4, 1, 2, 2, 3, 2, 6, 5, 3, 2, 4, 3, 6, 3, 3, 5, 5,\n","        2, 6, 3, 1, 3, 5, 3, 4, 5, 5, 4, 6, 0, 0, 0, 3, 6, 6, 5, 3, 0, 0, 0, 5,\n","        4, 5])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rx9OVCcn__88","colab_type":"code","colab":{}},"source":["# building the model\n","\n","import torch.nn as nn\n","\n","class SentimentLSTM(nn.Module):\n","    \"\"\"\n","    The RNN model that will be used to perform Sentiment analysis.\n","    \"\"\"\n","\n","    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n","        \"\"\"\n","        Initialize the model by setting up the layers.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","        \n","        # embedding and LSTM layers\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True, bidirectional=True)\n","        \n","        # dropout layer\n","        self.dropout = nn.Dropout(0.3)\n","        \n","        # linear and Relu layers\n","        self.fc = nn.Linear(hidden_dim*2, output_size)\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)\n","        \n","\n","    def forward(self, x, hidden):\n","        \"\"\"\n","        Perform a forward pass of our model on some input and hidden state.\n","        \"\"\"\n","        batch_size = x.size(0)\n","        seq_len = x.size(1)\n","\n","        # embeddings and lstm_out\n","        embeds = self.embedding(x)\n","        lstm_out, hidden = self.lstm(embeds, hidden)\n","    \n","         # get of labels from last node\n","        lstm_out = lstm_out[:, -1, :]\n","        \n","        # dropout and fully-connected layer\n","        out = self.dropout(lstm_out)\n","        out = self.fc(out)\n","        \n","        # Relu function\n","        relu_out = self.relu(out)\n","        \n","        # softmax\n","        softmax_out = self.softmax(relu_out)\n","        \n","        # return last Relu output and hidden state\n","        return softmax_out, hidden\n","    \n","    \n","    # To initialize the hidden state\n","    \n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        if (train_on_gpu):\n","            hidden = (weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().to(device),weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().to(device))\n","        else:\n","            hidden = (weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_(),weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_())\n","        \n","        return hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8GcJDT26A9u1","colab_type":"code","outputId":"d765be91-9f95-490d-9785-9b6b1b03f210","executionInfo":{"status":"ok","timestamp":1558935774228,"user_tz":-330,"elapsed":3137,"user":{"displayName":"GAGAN SINGH SAINI","photoUrl":"","userId":"08621075158641464912"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"source":["# Initiating the hyperparams\n","\n","vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n","output_size = 7\n","embedding_dim = 400\n","hidden_dim = 128\n","n_layers = 2\n","\n","# device to train on \n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","# Initiating model and model weights\n","\n","net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n","print(net)\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["cuda:0\n","SentimentLSTM(\n","  (embedding): Embedding(9340, 400)\n","  (lstm): LSTM(400, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n","  (dropout): Dropout(p=0.3)\n","  (fc): Linear(in_features=256, out_features=7, bias=True)\n","  (relu): ReLU()\n","  (softmax): Softmax()\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xGQrQWSDBos_","colab_type":"code","outputId":"6e83ea4f-7484-4b10-c12a-86dccabe94e9","executionInfo":{"status":"ok","timestamp":1558935891143,"user_tz":-330,"elapsed":120033,"user":{"displayName":"GAGAN SINGH SAINI","photoUrl":"","userId":"08621075158641464912"}},"colab":{"base_uri":"https://localhost:8080/","height":6283}},"source":["# Traing the model\n","\n","\n","import time\n","import os\n","import copy\n","\n","\n","since = time.time()\n","\n","best_model_wts = copy.deepcopy(net.state_dict())\n","best_acc = 0.0\n","\n","lr=0.001 # learning rate\n","\n","# Initiating optimizer\n","\n","import torch.optim as optim\n","optimizer = optim.Adam(net.parameters(),lr=lr)\n","\n","# definig loss function\n","\n","criterion = nn.CrossEntropyLoss()\n","criterion = criterion.to(device)\n","\n","# training params\n","\n","epochs = 50\n","counter = 0\n","print_every = 100\n","clip=5 # gradient clipping to prevent from gradients exploading \n","\n","train_on_gpu = True if torch.cuda.is_available() else False\n","loss = 0\n","\n","# move model to GPU, if available\n","if(train_on_gpu):\n","    net.cuda()\n","\n","# train for some number of epochs\n","\n","net.train()\n","for e in range(epochs):\n","  \n","    # initialize hidden state\n","    h = net.init_hidden(50)\n","    \n","    # for calculating train accuracy every epoch\n","    total = 0\n","    correct = 0\n","    \n","    # batch loop\n","    for inputs, labels in train_loader:\n","        counter += 1\n","\n","        if(train_on_gpu):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # converting cell and hidden state to tuple for feeding into LSTM layer\n","        h = tuple([each.data for each in h])\n","\n","        # Converting accumulated gradients to zero at starting of every epoch\n","        net.zero_grad()\n","      \n","      \n","        # get the output from the model\n","        inputs = inputs.type(torch.LongTensor)\n","        output, h = net(inputs.to(device), h)\n","        \n","        # getting the index of output tensors with max values\n","        _, predicted = torch.max(output.data, 1)\n","    \n","        # for calculating accuracy\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item() # comparing predictions to true label\n","        \n","        \n","        # calculate the loss and perform backprop\n","        loss = criterion(output, labels)\n","        loss.backward()\n","        \n","        # clip_grad_norm helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","\n","        # loss stats\n","        if counter % print_every == 0:\n","          \n","            # initialize hidden state for validation set\n","            val_h = net.init_hidden(50)\n","            \n","            total_val = 0\n","            correct_val = 0\n","            \n","            val_losses = [] # for tracking loss\n","            net.eval()\n","            for inputs, labels in valid_loader:\n","\n","                # converting cell and hidden state to tuple for feeding into LSTM layer\n","                val_h = tuple([each.data for each in val_h])\n","\n","                if(train_on_gpu):\n","                    inputs, labels = inputs.to(device), labels.to(device)\n","\n","                inputs = inputs.type(torch.LongTensor)\n","                output, val_h = net(inputs.to(device),val_h)\n","                \n","                # getting the index of output tensors with max values\n","                _, predicted_val = torch.max(output.data, 1)\n","    \n","                # for calculating accuracy\n","                total_val += labels.size(0)\n","                correct_val += (predicted_val == labels).sum().item() # comparing predictions to true label\n","\n","                val_loss = criterion(output, labels)\n","\n","                val_losses.append(val_loss.item())\n","\n","            net.train()\n","            val_acc = (correct_val / total_val) * 100\n","\n","            # deep copy the model\n","            if (val_acc > best_acc):\n","                best_acc = val_acc\n","                print(\"saving model .....\")\n","                best_model_wts = copy.deepcopy(net.state_dict())\n","            \n","            \n","            \n","            \n","            print(\"-\"*10)\n","            print(\"\")\n","            print(\"Epoch: {}/{}\".format(e+1, epochs),\n","                  \"Step: {}\".format(counter),\n","                  \"Loss: {:.4f}\".format(loss.item()),\n","                  \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n","            \n","            \n","            print(\"Train accuracy: {:.2f}\".format((correct / total) * 100), \"Val accuracy: {:.2f}\".format(val_acc))\n","            print(\"\")\n","            print(\"-\"*10)\n","            \n","            \n","time_elapsed = time.time() - since\n","print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","print('Best val Acc: {:4f}'.format(best_acc))"],"execution_count":22,"outputs":[{"output_type":"stream","text":["saving model .....\n","----------\n","\n","Epoch: 1/50 Step: 100 Loss: 1.8937 Val Loss: 1.8901\n","Train accuracy: 19.86 Val accuracy: 24.67\n","\n","----------\n","saving model .....\n","----------\n","\n","Epoch: 2/50 Step: 200 Loss: 1.6911 Val Loss: 1.8054\n","Train accuracy: 37.80 Val accuracy: 34.13\n","\n","----------\n","saving model .....\n","----------\n","\n","Epoch: 3/50 Step: 300 Loss: 1.7197 Val Loss: 1.7821\n","Train accuracy: 48.60 Val accuracy: 36.27\n","\n","----------\n","saving model .....\n","----------\n","\n","Epoch: 4/50 Step: 400 Loss: 1.6882 Val Loss: 1.7572\n","Train accuracy: 58.15 Val accuracy: 39.87\n","\n","----------\n","----------\n","\n","Epoch: 5/50 Step: 500 Loss: 1.6152 Val Loss: 1.7660\n","Train accuracy: 59.80 Val accuracy: 39.33\n","\n","----------\n","saving model .....\n","----------\n","\n","Epoch: 5/50 Step: 600 Loss: 1.6219 Val Loss: 1.7222\n","Train accuracy: 64.27 Val accuracy: 44.27\n","\n","----------\n","----------\n","\n","Epoch: 6/50 Step: 700 Loss: 1.4687 Val Loss: 1.7241\n","Train accuracy: 71.10 Val accuracy: 42.93\n","\n","----------\n","saving model .....\n","----------\n","\n","Epoch: 7/50 Step: 800 Loss: 1.3068 Val Loss: 1.7054\n","Train accuracy: 76.08 Val accuracy: 45.20\n","\n","----------\n","saving model .....\n","----------\n","\n","Epoch: 8/50 Step: 900 Loss: 1.4960 Val Loss: 1.6986\n","Train accuracy: 79.17 Val accuracy: 45.87\n","\n","----------\n","saving model .....\n","----------\n","\n","Epoch: 9/50 Step: 1000 Loss: 1.2859 Val Loss: 1.6918\n","Train accuracy: 81.65 Val accuracy: 46.80\n","\n","----------\n","----------\n","\n","Epoch: 10/50 Step: 1100 Loss: 1.2531 Val Loss: 1.6905\n","Train accuracy: 83.30 Val accuracy: 46.53\n","\n","----------\n","----------\n","\n","Epoch: 10/50 Step: 1200 Loss: 1.2841 Val Loss: 1.7013\n","Train accuracy: 83.40 Val accuracy: 46.13\n","\n","----------\n","----------\n","\n","Epoch: 11/50 Step: 1300 Loss: 1.2721 Val Loss: 1.6981\n","Train accuracy: 85.18 Val accuracy: 45.60\n","\n","----------\n","saving model .....\n","----------\n","\n","Epoch: 12/50 Step: 1400 Loss: 1.2456 Val Loss: 1.6875\n","Train accuracy: 85.38 Val accuracy: 47.47\n","\n","----------\n","----------\n","\n","Epoch: 13/50 Step: 1500 Loss: 1.3148 Val Loss: 1.7020\n","Train accuracy: 86.23 Val accuracy: 45.33\n","\n","----------\n","----------\n","\n","Epoch: 14/50 Step: 1600 Loss: 1.2516 Val Loss: 1.6900\n","Train accuracy: 88.10 Val accuracy: 47.20\n","\n","----------\n","----------\n","\n","Epoch: 15/50 Step: 1700 Loss: 1.2349 Val Loss: 1.6936\n","Train accuracy: 87.30 Val accuracy: 46.67\n","\n","----------\n","----------\n","\n","Epoch: 15/50 Step: 1800 Loss: 1.2444 Val Loss: 1.6952\n","Train accuracy: 88.12 Val accuracy: 46.27\n","\n","----------\n","----------\n","\n","Epoch: 16/50 Step: 1900 Loss: 1.2235 Val Loss: 1.7078\n","Train accuracy: 88.54 Val accuracy: 44.67\n","\n","----------\n","----------\n","\n","Epoch: 17/50 Step: 2000 Loss: 1.2677 Val Loss: 1.6948\n","Train accuracy: 88.05 Val accuracy: 46.67\n","\n","----------\n","----------\n","\n","Epoch: 18/50 Step: 2100 Loss: 1.2681 Val Loss: 1.7019\n","Train accuracy: 89.17 Val accuracy: 45.20\n","\n","----------\n","----------\n","\n","Epoch: 19/50 Step: 2200 Loss: 1.2568 Val Loss: 1.7066\n","Train accuracy: 89.30 Val accuracy: 45.33\n","\n","----------\n","----------\n","\n","Epoch: 20/50 Step: 2300 Loss: 1.2035 Val Loss: 1.7117\n","Train accuracy: 90.70 Val accuracy: 44.93\n","\n","----------\n","----------\n","\n","Epoch: 20/50 Step: 2400 Loss: 1.2452 Val Loss: 1.6936\n","Train accuracy: 89.82 Val accuracy: 46.93\n","\n","----------\n","----------\n","\n","Epoch: 21/50 Step: 2500 Loss: 1.2248 Val Loss: 1.7093\n","Train accuracy: 90.18 Val accuracy: 44.80\n","\n","----------\n","----------\n","\n","Epoch: 22/50 Step: 2600 Loss: 1.3570 Val Loss: 1.7021\n","Train accuracy: 90.62 Val accuracy: 45.87\n","\n","----------\n","----------\n","\n","Epoch: 23/50 Step: 2700 Loss: 1.2172 Val Loss: 1.6944\n","Train accuracy: 90.90 Val accuracy: 46.27\n","\n","----------\n","----------\n","\n","Epoch: 24/50 Step: 2800 Loss: 1.3247 Val Loss: 1.6948\n","Train accuracy: 90.40 Val accuracy: 46.40\n","\n","----------\n","saving model .....\n","----------\n","\n","Epoch: 25/50 Step: 2900 Loss: 1.2473 Val Loss: 1.6782\n","Train accuracy: 91.20 Val accuracy: 48.40\n","\n","----------\n","----------\n","\n","Epoch: 25/50 Step: 3000 Loss: 1.1941 Val Loss: 1.7023\n","Train accuracy: 90.87 Val accuracy: 45.47\n","\n","----------\n","----------\n","\n","Epoch: 26/50 Step: 3100 Loss: 1.2655 Val Loss: 1.6968\n","Train accuracy: 91.04 Val accuracy: 46.40\n","\n","----------\n","----------\n","\n","Epoch: 27/50 Step: 3200 Loss: 1.2324 Val Loss: 1.7001\n","Train accuracy: 91.03 Val accuracy: 46.40\n","\n","----------\n","----------\n","\n","Epoch: 28/50 Step: 3300 Loss: 1.2649 Val Loss: 1.6968\n","Train accuracy: 90.83 Val accuracy: 46.27\n","\n","----------\n","----------\n","\n","Epoch: 29/50 Step: 3400 Loss: 1.3235 Val Loss: 1.6850\n","Train accuracy: 90.90 Val accuracy: 47.47\n","\n","----------\n","----------\n","\n","Epoch: 30/50 Step: 3500 Loss: 1.3037 Val Loss: 1.6779\n","Train accuracy: 92.60 Val accuracy: 48.00\n","\n","----------\n","----------\n","\n","Epoch: 30/50 Step: 3600 Loss: 1.2661 Val Loss: 1.6812\n","Train accuracy: 91.67 Val accuracy: 47.60\n","\n","----------\n","----------\n","\n","Epoch: 31/50 Step: 3700 Loss: 1.1865 Val Loss: 1.6898\n","Train accuracy: 91.64 Val accuracy: 46.93\n","\n","----------\n","----------\n","\n","Epoch: 32/50 Step: 3800 Loss: 1.2573 Val Loss: 1.6893\n","Train accuracy: 91.70 Val accuracy: 47.20\n","\n","----------\n","----------\n","\n","Epoch: 33/50 Step: 3900 Loss: 1.1889 Val Loss: 1.6940\n","Train accuracy: 91.73 Val accuracy: 46.40\n","\n","----------\n","----------\n","\n","Epoch: 34/50 Step: 4000 Loss: 1.2845 Val Loss: 1.6942\n","Train accuracy: 91.30 Val accuracy: 46.53\n","\n","----------\n","----------\n","\n","Epoch: 35/50 Step: 4100 Loss: 1.2644 Val Loss: 1.6972\n","Train accuracy: 92.10 Val accuracy: 46.67\n","\n","----------\n","----------\n","\n","Epoch: 35/50 Step: 4200 Loss: 1.1702 Val Loss: 1.7066\n","Train accuracy: 91.98 Val accuracy: 44.67\n","\n","----------\n","----------\n","\n","Epoch: 36/50 Step: 4300 Loss: 1.2065 Val Loss: 1.6933\n","Train accuracy: 92.08 Val accuracy: 46.67\n","\n","----------\n","----------\n","\n","Epoch: 37/50 Step: 4400 Loss: 1.2263 Val Loss: 1.7001\n","Train accuracy: 92.42 Val accuracy: 45.73\n","\n","----------\n","----------\n","\n","Epoch: 38/50 Step: 4500 Loss: 1.2058 Val Loss: 1.7014\n","Train accuracy: 92.03 Val accuracy: 45.60\n","\n","----------\n","----------\n","\n","Epoch: 39/50 Step: 4600 Loss: 1.2063 Val Loss: 1.7047\n","Train accuracy: 92.35 Val accuracy: 44.67\n","\n","----------\n","----------\n","\n","Epoch: 40/50 Step: 4700 Loss: 1.2055 Val Loss: 1.6892\n","Train accuracy: 93.30 Val accuracy: 47.33\n","\n","----------\n","----------\n","\n","Epoch: 40/50 Step: 4800 Loss: 1.2146 Val Loss: 1.6852\n","Train accuracy: 92.22 Val accuracy: 47.07\n","\n","----------\n","----------\n","\n","Epoch: 41/50 Step: 4900 Loss: 1.2708 Val Loss: 1.6925\n","Train accuracy: 92.10 Val accuracy: 47.07\n","\n","----------\n","----------\n","\n","Epoch: 42/50 Step: 5000 Loss: 1.2073 Val Loss: 1.6866\n","Train accuracy: 92.47 Val accuracy: 46.80\n","\n","----------\n","----------\n","\n","Epoch: 43/50 Step: 5100 Loss: 1.2106 Val Loss: 1.6930\n","Train accuracy: 92.67 Val accuracy: 46.40\n","\n","----------\n","----------\n","\n","Epoch: 44/50 Step: 5200 Loss: 1.2044 Val Loss: 1.6862\n","Train accuracy: 92.65 Val accuracy: 47.73\n","\n","----------\n","----------\n","\n","Epoch: 45/50 Step: 5300 Loss: 1.2054 Val Loss: 1.6865\n","Train accuracy: 92.90 Val accuracy: 47.20\n","\n","----------\n","----------\n","\n","Epoch: 45/50 Step: 5400 Loss: 1.2533 Val Loss: 1.6884\n","Train accuracy: 93.00 Val accuracy: 47.07\n","\n","----------\n","saving model .....\n","----------\n","\n","Epoch: 46/50 Step: 5500 Loss: 1.1665 Val Loss: 1.6722\n","Train accuracy: 93.46 Val accuracy: 49.47\n","\n","----------\n","----------\n","\n","Epoch: 47/50 Step: 5600 Loss: 1.2829 Val Loss: 1.6793\n","Train accuracy: 93.38 Val accuracy: 47.87\n","\n","----------\n","----------\n","\n","Epoch: 48/50 Step: 5700 Loss: 1.2276 Val Loss: 1.6743\n","Train accuracy: 93.60 Val accuracy: 48.40\n","\n","----------\n","----------\n","\n","Epoch: 49/50 Step: 5800 Loss: 1.2054 Val Loss: 1.6915\n","Train accuracy: 93.30 Val accuracy: 46.93\n","\n","----------\n","----------\n","\n","Epoch: 50/50 Step: 5900 Loss: 1.1780 Val Loss: 1.6780\n","Train accuracy: 93.40 Val accuracy: 48.00\n","\n","----------\n","----------\n","\n","Epoch: 50/50 Step: 6000 Loss: 1.1871 Val Loss: 1.6871\n","Train accuracy: 93.28 Val accuracy: 47.47\n","\n","----------\n","Training complete in 1m 57s\n","Best val Acc: 49.466667\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OCOKsm7uyPuY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c2ca5bcd-5f2a-4246-a142-6afc419f26bc","executionInfo":{"status":"ok","timestamp":1558935891144,"user_tz":-330,"elapsed":120019,"user":{"displayName":"GAGAN SINGH SAINI","photoUrl":"","userId":"08621075158641464912"}}},"source":["net.load_state_dict(best_model_wts)"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["IncompatibleKeys(missing_keys=[], unexpected_keys=[])"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"0Dz1kk58EVOt","colab_type":"code","outputId":"0ba743a5-a652-45d1-a029-4e6d688fa7d7","executionInfo":{"status":"ok","timestamp":1558935891145,"user_tz":-330,"elapsed":120004,"user":{"displayName":"GAGAN SINGH SAINI","photoUrl":"","userId":"08621075158641464912"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["test_losses = [] # for tracking loss\n","\n","\n","# init hidden state\n","h = net.init_hidden(batch_size)\n","total = 0\n","correct = 0\n","net.eval()\n","\n","# iterate over test data\n","for inputs, labels in test_loader:\n","\n","    h = tuple([each.data for each in h])\n","\n","    if(train_on_gpu):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","    # get predicted outputs\n","    inputs = inputs.type(torch.LongTensor)\n","    output, h = net(inputs.to(device), h)\n","    \n","    # getting the index of output tensors with max values\n","    _, predicted = torch.max(output.data, 1)\n","    \n","    # for calculating accuracy\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item() # comparing predictions to true label\n","\n","    \n","    test_loss = criterion(output, labels)\n","    test_losses.append(test_loss.item())\n","    \n","print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))    \n","print('Accuracy of the network on the test data : {:.3f} %'.format((correct / total) * 100))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Test loss: 1.586\n","Accuracy of the network on the test data : 57.467 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Cvuz7fikF4qb","colab_type":"code","outputId":"c1d2c188-8ec6-426f-ba71-b6be628342f7","executionInfo":{"status":"ok","timestamp":1558935926960,"user_tz":-330,"elapsed":1072,"user":{"displayName":"GAGAN SINGH SAINI","photoUrl":"","userId":"08621075158641464912"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["# testing on custom dataset\n","\n","from string import punctuation\n","\n","# for tranforming the string into tokens\n","\n","def tokenize_text(test_text):\n","    test_text = test_text.lower() # lowercase\n","    \n","    # get rid of punctuation\n","    test_text = ''.join([c for c in test_text if c not in punctuation])\n","\n","    # splitting the lines into words for tokenization\n","    test_words = test_text.split()\n","\n","    # tokenizing the text\n","    test_ints = []\n","    test_ints.append([vocab_to_int[word] for word in test_words])\n","\n","    return test_ints\n","  \n","# test string\n","test_text = \"I am very angry !! :)\"\n","\n","\n","def predict(net, test_text, sequence_length=50):\n","    \n","    net.eval()\n","    \n","    # tokenize text\n","    test_ints = tokenize_text(test_text)\n","    \n","    # pad tokenized sequence\n","    seq_length=sequence_length\n","    features = pad_features(test_ints, seq_length)\n","    \n","    # convering the array to tensor to pass into the model\n","    feature_tensor = torch.from_numpy(features)\n","    \n","    batch_size = feature_tensor.size(0) # number of strings to test\n","\n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","    \n","    if(train_on_gpu):\n","        feature_tensor = feature_tensor.to(device)\n","    \n","    # get the output from the model\n","    output, h = net(feature_tensor, h)\n","    print(output.cpu().detach().numpy())\n","    print(emotions)\n","\n","    _, predicted = torch.max(output.data, 1)\n","    print(\"{} --> {}\".format(test_text, emotions[predicted.cpu().numpy()[0]]))\n","\n","    \n","predict(net, test_text, sequence_length=50)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["[[7.2643797e-06 7.2643797e-06 9.9994898e-01 7.2643797e-06 7.2643797e-06\n","  7.2643797e-06 1.4689908e-05]]\n","['joy', 'fear', 'anger', 'sadness', 'disgust', 'shame', 'guilt']\n","I am very angry !! :) --> anger\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VpIBhGb5Lvm5","colab_type":"code","colab":{}},"source":["#torch.save(net.state_dict(), 'model_sentiment analysis_v1.pt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WoTnGT2oOdEz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}