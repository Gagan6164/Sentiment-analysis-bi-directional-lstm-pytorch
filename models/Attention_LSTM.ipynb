{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "root_path = 'gdrive/My Drive/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.) function for reading data and formatting\n",
    "\n",
    "def read_file(file_name): \n",
    "    data_list  = []\n",
    "    with open(file_name, 'r') as f: \n",
    "        for line in f: \n",
    "            line = line.strip() \n",
    "            label = ' '.join(line[1:line.find(\"]\")].strip().split())\n",
    "            text = line[line.find(\"]\")+1:].strip()\n",
    "            data_list.append([label, text])\n",
    "    return data_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path\n",
    "\n",
    "file_name = \"../content/psychExp.txt\"\n",
    "psychExp_txt = read_file(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total no. of instances: {}'.format(len(psychExp_txt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for converting labels from one hot to classes\n",
    "\n",
    "def convert_label(item, name): \n",
    "    items = list(map(float, item.split()))\n",
    "    label = \"\"\n",
    "    for idx in range(len(items)): \n",
    "        if items[idx] == 1: \n",
    "            label += name[idx] + \" \"\n",
    "    \n",
    "    return label.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes\n",
    "\n",
    "emotions = [\"joy\", 'fear', \"anger\", \"sadness\", \"disgust\", \"shame\", \"guilt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = []\n",
    "y_all = []\n",
    "for label, text in psychExp_txt:\n",
    "    X_all.append(text.lower())\n",
    "    y_all.append(convert_label(label, emotions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "\n",
    "from string import punctuation\n",
    "print('punctuations to be removed: {}'.format(punctuation))\n",
    "\n",
    "X_all_pro = []\n",
    "for text in X_all:\n",
    "    all_text = ''.join([c for c in text if c not in punctuation])\n",
    "    X_all_pro.append(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize — Create Vocab to Int mapping dictionary\n",
    "\n",
    "from collections import Counter\n",
    "all_text2 = ' '.join(X_all_pro)\n",
    "\n",
    "# create a list of words\n",
    "words = all_text2.split()\n",
    "\n",
    "# Count all the words using Counter Method\n",
    "count_words = Counter(words)\n",
    "\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted_words.insert(0,('',11000))\n",
    "print(len(sorted_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab to integer mapping dictionary with starting index 1\n",
    "\n",
    "vocab_to_int = {w:i for i, (w,c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = []\n",
    "for i, (w,c) in enumerate(sorted_words):\n",
    "    target_vocab.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "glove_path = \"../content/gdrive/My Drive/\"\n",
    "store_dir = \"../content\"\n",
    "vectors = bcolz.carray(np.zeros(1), rootdir=f'{store_dir}/6B.300.dat', mode='w')\n",
    "\n",
    "with open(f'{glove_path}/glove.6B.300d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors.append(vect)\n",
    "    \n",
    "vectors = bcolz.carray(vectors[1:].reshape((400000, 300)), rootdir=f'{store_dir}/6B.300.dat', mode='w')\n",
    "vectors.flush()\n",
    "pickle.dump(words, open(f'{store_dir}/6B.300_words.pkl', 'wb'))\n",
    "pickle.dump(word2idx, open(f'{store_dir}/6B.300_idx.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "store_dir = \"../content\"\n",
    "vectors = bcolz.open(f'{store_dir}/6B.300.dat')[:]\n",
    "words = pickle.load(open(f'{store_dir}/6B.300_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(f'{store_dir}/6B.300_idx.pkl', 'rb'))\n",
    "\n",
    "glove = {w: vectors[word2idx[w]] for w in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matrix_len = len(target_vocab)\n",
    "weights_matrix = np.zeros((matrix_len, 300))\n",
    "words_found = 0\n",
    "emb_dim = 300\n",
    "\n",
    "for i, word in enumerate(target_vocab):\n",
    "    try: \n",
    "        weights_matrix[i] = glove[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoading the words\n",
    "\n",
    "X_all_int = []\n",
    "for text in X_all_pro:\n",
    "    r = [vocab_to_int[w] for w in text.split()]\n",
    "    X_all_int.append(r)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convering labels from classes to class index\n",
    "def label_to_int(label_text):\n",
    "    emotions_dict = {\"joy\":0, \"fear\":1, \"anger\":2, \"sadness\":3, \"disgust\":4, \"shame\":5, \"guilt\":6}\n",
    "    label_int = []\n",
    "    for label in label_text:\n",
    "        label_int.append(emotions_dict[label])\n",
    "    return label_int\n",
    "  \n",
    "labels = label_to_int(y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysing the text data lenghth\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "text_len = [len(x) for x in X_all_int]\n",
    "pd.Series(text_len).hist()\n",
    "plt.show()\n",
    "pd.Series(text_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the data for having same dimention\n",
    "import numpy as np\n",
    "\n",
    "def pad_features(text_int, seq_length):\n",
    "    ''' Return features of text_ints, where each text is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    features = np.zeros((len(text_int), seq_length), dtype = int)\n",
    "    \n",
    "    for i, text in enumerate(text_int):\n",
    "        text_len = len(text)\n",
    "        \n",
    "        if text_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-text_len))\n",
    "            new = zeroes+text\n",
    "        elif text_len > seq_length:\n",
    "            new = text[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 64 # length after padding \n",
    "X_all_pad = pad_features(X_all_int, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('length of X_all_pad: {}'.format(len(X_all_pad)))\n",
    "print('length of labels: {}'.format(len(labels)))\n",
    "\n",
    "# converting labels from list to numpy.nparray\n",
    "\n",
    "print('data type of labels before convesion: {}'.format(type(labels)))\n",
    "labels = np.array(labels)\n",
    "print('data type of labels after convesion: {}'.format(type(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train (80%), validation (10%) ,test data (10%) split\n",
    "\n",
    "len_X_all = len(X_all_pad)\n",
    "\n",
    "\n",
    "split_frac = 0.8\n",
    "\n",
    "train_x = X_all_pad[0:int(split_frac*len_X_all)+16]\n",
    "train_y = labels[0:int(split_frac*len_X_all)+16]\n",
    "\n",
    "remaining_x = X_all_pad[int(split_frac*len_X_all)-4:]\n",
    "remaining_y = labels[int(split_frac*len_X_all)-4:]\n",
    "\n",
    "test_x = remaining_x[0:int(len(remaining_x)*0.5)]\n",
    "test_y = remaining_y[0:int(len(remaining_y)*0.5)]\n",
    "\n",
    "valid_x = remaining_x[int(len(remaining_x)*0.5):]\n",
    "valid_y = remaining_y[int(len(remaining_y)*0.5):]\n",
    "\n",
    "print(type(train_x),type(np.array(train_y)))\n",
    "\n",
    "print(\"train data size: \",train_x.shape)\n",
    "print(\"remaining data size: \",remaining_x.shape)\n",
    "print(\"valid data size: \",valid_x.shape)\n",
    "print(\"test data size: \",test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and batch formation\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# Shuffeling the data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training data\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=True):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    #emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    emb_layer.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import Logger\n",
    "\n",
    "logger = Logger('../content/logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,weights_matrix, output_size, hidden_dim, n_layers ):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers   \n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and Softmax layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        \n",
    "        hidden = final_state.squeeze(0)\n",
    "        attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        return new_hidden_state\n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "         # get of labels from last node\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        #output = output.permute(1, 0, 2) # output.size() = (batch_size, num_seq, hidden_size)\n",
    "        \n",
    "        attn_output = self.attention_net(lstm_out, hidden[0])\n",
    "        logits = self.fc(attn_output)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating the hyperparams\n",
    " \n",
    "output_size = 7\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "# device to train on \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Initiating model and model weights\n",
    "\n",
    "net = AttentionLSTM(weights_matrix, output_size, hidden_dim, n_layers)\n",
    "net.to(device)\n",
    "print(net)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
